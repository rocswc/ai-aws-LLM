# save as: rag_with_sft_cli.py

import os
import torch
import chromadb
from sentence_transformers import SentenceTransformer

from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    pipeline,
)

from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.llms import HuggingFacePipeline
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA


# ===============================
# í™˜ê²½ ì„¤ì • (ë„¤ í™˜ê²½ ê·¸ëŒ€ë¡œ)
# ===============================
CHROMA_DIR = "/opt/dlami/nvme/chroma_store_hybrid"
COLLECTION_NAME = "pdf_hybrid_collection"

BASE_MODEL = "Qwen/Qwen2.5-3B-Instruct"
LORA_PATH = "/opt/dlami/nvme/sft_lora_qwen"

EMBED_MODEL = "intfloat/multilingual-e5-large"
DEVICE = "cuda"

os.environ["HF_HOME"] = "/opt/dlami/nvme/hf_cache"


# ===============================
# Embedding (Chromaì™€ ë™ì¼í•´ì•¼ í•¨)
# ===============================
embeddings = HuggingFaceEmbeddings(
    model_name=EMBED_MODEL,
    model_kwargs={"device": DEVICE},
    encode_kwargs={"normalize_embeddings": True},
)


# ===============================
# VectorStore (ê¸°ì¡´ Chroma ìž¬ì‚¬ìš©)
# ===============================
vectorstore = Chroma(
    persist_directory=CHROMA_DIR,
    collection_name=COLLECTION_NAME,
    embedding_function=embeddings,
)

retriever = vectorstore.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 5},
)


# ===============================
# SFT + LoRA ëª¨ë¸ ë¡œë“œ
# ===============================
tokenizer = AutoTokenizer.from_pretrained(
    BASE_MODEL,
    trust_remote_code=True,
)

model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL,
    torch_dtype=torch.float16,
    device_map="cuda",
    trust_remote_code=True,
)

# LoRA ì–´ëŒ‘í„° ì ìš©
model.load_adapter(LORA_PATH)
model.eval()

pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=512,
    do_sample=False,
    return_full_text=False,
)

llm = HuggingFacePipeline(pipeline=pipe)


# ===============================
# ðŸ”¥ í•µì‹¬: ìˆ˜ì •ëœ í”„ë¡¬í”„íŠ¸
# ===============================
PROMPT = PromptTemplate(
    input_variables=["context", "question"],
    template="""
ë„ˆëŠ” í–‰ì • ë¬¸ì„œë¥¼ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ë‹¤.

ê·œì¹™:
- ë‹µë³€ì€ ë°˜ë“œì‹œ ì œê³µëœ ë¬¸ì„œ(context)ì— ê·¼ê±°í•´ì•¼ í•œë‹¤.
- ë¬¸ì„œì— í‘œë‚˜ ìˆ˜ì¹˜ê°€ ìžˆëŠ” ê²½ìš°,
  ë¹„êµ, ê³„ì‚°, í•´ì„ì„ í†µí•´ ê²°ë¡ ì„ ë„ì¶œí•˜ëŠ” ê²ƒì„ í—ˆìš©í•œë‹¤.
- ë¬¸ì„œì— ì „í˜€ ê·¼ê±°ê°€ ì—†ëŠ” ê²½ìš°ì—ë§Œ
  "ë¬¸ì„œì— í•´ë‹¹ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ë‹µí•˜ë¼.
- ê°™ì€ ë¬¸êµ¬ë¥¼ ë°˜ë³µí•˜ì§€ ë§ê³  í•˜ë‚˜ì˜ ê²°ë¡ ìœ¼ë¡œ ë‹µí•˜ë¼.
- ë¶ˆí•„ìš”í•œ ì›ë¬¸ ë³µë¶™ì€ ê¸ˆì§€í•œë‹¤.

[ë¬¸ì„œ]
{context}

[ì§ˆë¬¸]
{question}

[ë‹µë³€]
""".strip(),
)


# ===============================
# RetrievalQA Chain
# ===============================
qa = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    chain_type_kwargs={"prompt": PROMPT},
    return_source_documents=True,
)


# ===============================
# CLI ì¸í„°íŽ˜ì´ìŠ¤
# ===============================
def main():
    print("\nâœ… RAG + SFT QA ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ")
    print("ì§ˆë¬¸ì„ ìž…ë ¥í•˜ì„¸ìš”. (ì¢…ë£Œ: exit / quit)\n")

    while True:
        query = input("â“ ì§ˆë¬¸ > ").strip()
        if query.lower() in ("exit", "quit"):
            print("ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        result = qa.invoke({"query": query})

        print("\nðŸ“Œ ë‹µë³€:")
        print(result["result"])

        print("\nðŸ“Œ ê·¼ê±° ë¬¸ì„œ:")
        seen_pages = set()
        for doc in result["source_documents"]:
            page = doc.metadata.get("page")
            if page not in seen_pages:
                print(f"- page={page}")
                seen_pages.add(page)

        print("\n" + "=" * 60 + "\n")


if __name__ == "__main__":
    main()

